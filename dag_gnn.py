# -*- coding: utf-8 -*-
"""DAG GNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yRQ1X1OlT3zjJ9HyjNubBninzwr2VUk1
"""

pip install bnlearn

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import bnlearn as bn
import matplotlib.pyplot as plt
import torch.nn.functional as F


def normalize_adjacency_matrix(A):
    """Normalizes adjacency matrix."""
    I = torch.eye(A.size(0), device=A.device)
    A_hat = A + I
    D_hat = torch.diag(torch.sum(A_hat, axis=1))
    return torch.linalg.inv(D_hat) @ A_hat

def plot_kl_loss(kl_losses):
    """Plots the KL loss."""
    epochs = range(1, len(kl_losses) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, kl_losses, label='KL Loss', color='blue')
    plt.xlabel('Epochs')
    plt.ylabel('KL Loss')
    plt.title('KL Loss Over Training')
    plt.legend()
    plt.grid()
    plt.show()

def plot_nll_loss(nll_losses):
    """Plots the NLL loss."""
    epochs = range(1, len(nll_losses) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, nll_losses, label='NLL Loss', color='green')
    plt.xlabel('Epochs')
    plt.ylabel('NLL Loss')
    plt.title('NLL Loss Over Training')
    plt.legend()
    plt.grid()
    plt.show()

def plot_acyclic_constraint(acyclic_constraints):
    """Plots the Acyclic Constraint."""
    epochs = range(1, len(acyclic_constraints) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, acyclic_constraints, label='Acyclic Constraint', color='red')
    plt.xlabel('Epochs')
    plt.ylabel('Acyclic Constraint')
    plt.title('Acyclic Constraint Over Training')
    plt.legend()
    plt.grid()
    plt.show()

def threshold_adjacency_matrix(A, threshold=0.5):
    """Applies a threshold to create a binary adjacency matrix."""
    A_binary = (A > threshold).float()
    return A_binary



class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(Encoder, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

    def forward(self, X, A):
        A_hat = normalize_adjacency_matrix(1 - A.T)
        Z = A_hat @ self.mlp(X).T
        Z = torch.sigmoid(Z)
        return Z

class Decoder(nn.Module):
    def __init__(self, latent_dim, hidden_dim, output_dim):
        super(Decoder, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Sigmoid()
        )

    def forward(self, Z, A):
        A_hat = torch.linalg.inv(1 - A.T)
        X_hat = self.mlp((A_hat @ Z).T)
        return X_hat

class DAGGNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(DAGGNN, self).__init__()
        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)
        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)

    def forward(self, X, A):
        Z = self.encoder(X, A)
        X_hat = self.decoder(Z, A)
        return Z, X_hat


def kl_bernoulli(preds):
    """KL Divergence for Bernoulli latent variables."""
    prior = 0.5  # Assuming Bernoulli prior P(z=1)=0.5
    kl_div = preds * torch.log(preds / prior + 1e-8) + (1 - preds) * torch.log((1 - preds) / (1 - prior) + 1e-8)
    return kl_div.sum() / preds.size(0)

def binary_nll(preds, target):
    """Binary Cross-Entropy Loss for Binary Data."""
    bce_loss = nn.BCELoss()  # Built-in PyTorch Binary Cross-Entropy Loss
    return bce_loss(preds, target)

def acyclicity_constraint(A):
    """Computes the acyclicity constraint."""
    M = torch.eye(A.size(0), device=A.device) + A * A
    return torch.trace(torch.matrix_power(M, A.size(0))) - A.size(0)

# Hyperparameters
input_dim = 10
hidden_dim = 64
latent_dim = 10
learning_rate = 0.001
num_epochs = 500

# data prepocessing
X = bn.import_example('asia')
X = torch.from_numpy(X.values).float()

input_dim = 8
latent_dim = 8
A = torch.rand(input_dim, input_dim)  # Random adjacency matrix
A.requires_grad = True

# Model, optimizer
model = DAGGNN(input_dim, hidden_dim, latent_dim)
optimizer = optim.Adam(list(model.parameters()) + [A], lr=learning_rate)

# Initialize lists to store losses
kl_losses = []
nll_losses = []
acyclic_constraints = []


# Training loop
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    Z, X_hat = model(X, A)
    kl_loss = kl_bernoulli(Z)
    nll_loss = binary_nll(X_hat, X)
    acyclic_loss = acyclicity_constraint(A)

    loss = kl_loss + nll_loss + 0.001 * acyclic_loss

    # Store losses
    kl_losses.append(kl_loss.item())
    nll_losses.append(nll_loss.item())
    acyclic_constraints.append(acyclic_loss.item())

    # Backpropagation and optimization
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, KL: {kl_loss.item():.4f}, NLL: {nll_loss.item():.4f}, Acyclic: {acyclic_loss.item():.4f}")

print("Training complete.")

# Plot the losses separately
final_A_binary = threshold_adjacency_matrix(A.detach(), threshold=0.7)
print("Final Learned Binary Adjacency Matrix:")
print(final_A_binary)
plot_kl_loss(kl_losses)
plot_nll_loss(nll_losses)
plot_acyclic_constraint(acyclic_constraints)